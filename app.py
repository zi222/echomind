import asyncio
import websockets
import os
import glob
from chat_with_wenwu.streamlit_app import get_qa_history_chain, gen_response, get_available_models
from TTS.tts import text_to_speech, initialize_tts_models
from pathlib import Path

# ç¼“å­˜é—®ç­”é“¾
_qa_chain = None
_selected_model = "qwen2.5-coder-32b-instruct"
_chat_history = []

def clean_audio_data_folder():
    audio_files = glob.glob("data/*.wav")
    for f in audio_files:
        try:
            os.remove(f)
        except Exception as e:
            print(f"Failed to delete {f}: {e}")

async def send_audio_to_cloud(audio_path):
    uri = "ws://182.92.128.19:8000/ws/upload"
    CHUNK_SIZE = 512 * 1024

    try:
        async with websockets.connect(uri, max_size=50 * 1024 * 1024) as websocket:
            file_size = os.path.getsize(audio_path)
            await websocket.send(f"{os.path.basename(audio_path)},{file_size}")

            with open(audio_path, "rb") as f:
                bytes_sent = 0
                while bytes_sent < file_size:
                    chunk = f.read(CHUNK_SIZE)
                    await websocket.send(chunk)
                    bytes_sent += len(chunk)

        print(f"âœ… éŸ³é¢‘å·²å‘é€è‡³æœåŠ¡å™¨ï¼š{audio_path}")

        # å‘é€æˆåŠŸï¼Œåˆ é™¤éŸ³é¢‘
        os.remove(audio_path)
        print(f"ðŸ—‘ï¸ å·²åˆ é™¤æœ¬åœ°éŸ³é¢‘æ–‡ä»¶ï¼š{audio_path}")
    except asyncio.TimeoutError:
        print("Sending keepalive ping...")
        await websocket.ping() # å‘é€å¿ƒè·³åŒ…ï¼Œä¿æŒè¿žæŽ¥çŠ¶æ€
    except Exception as e:
        print(f"âŒ WebSocket å‘é€å¤±è´¥ï¼š{e}ï¼ˆéŸ³é¢‘ä¿ç•™ä»¥ä¾¿é‡è¯•ï¼‰")

def get_available_pdfs():
    """èŽ·å–knowledge_dbç›®å½•ä¸‹æ‰€æœ‰PDFæ–‡ä»¶"""
    knowledge_dir = Path("/root/codespace/data/knowledge_db")
    knowledge_dir.mkdir(parents=True, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
    return list(knowledge_dir.glob("*.pdf"))

def load_qa_chain(model="qwen2.5-coder-32b-instruct", vectordb_path=None):
    global _qa_chain, _vectordb_path
    _qa_chain = get_qa_history_chain(model=model, vectordb_path=vectordb_path)
    print(f"QA chain loaded with model {model}")

def initialize_models():
    print("åˆå§‹åŒ– TTS æ¨¡åž‹...")
    # è®¾ç½®å†…å­˜ä¼˜åŒ–é…ç½®
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
    # åˆå§‹åŒ–TTSæ¨¡åž‹ï¼Œå…³é—­halfç²¾åº¦ä»¥å‡å°‘å†…å­˜é—®é¢˜
    initialize_tts_models(half=False)
    clean_audio_data_folder()
    load_qa_chain(model=_selected_model, vectordb_path=None)

async def process_text(text):
    # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„PDFçŸ¥è¯†åº“
    pdf_files = get_available_pdfs()
    if pdf_files:
        pdf_file = pdf_files[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªPDFæ–‡ä»¶
        pdf_path = str(pdf_file)
        persist_dir = Path("/root/codespace/data/vector_db") / pdf_file.stem
        persist_dir.mkdir(parents=True, exist_ok=True)
        vectordb_path = (pdf_path, str(persist_dir))
        
            
        print(f"æ£€æµ‹åˆ°çŸ¥è¯†åº“æ–‡ä»¶ï¼š{pdf_path}ï¼Œæ›´æ–°QAé“¾...")
        try:
            load_qa_chain(model=_selected_model, vectordb_path=vectordb_path)
        except Exception as e:
            print(f"åŠ è½½QAé“¾å¤±è´¥: {e}")
            print("å°†ä½¿ç”¨æ— çŸ¥è¯†åº“æ¨¡å¼ç»§ç»­...")
            load_qa_chain(model=_selected_model, vectordb_path=None)
    else:
        print("æœªæ£€æµ‹åˆ°çŸ¥è¯†åº“æ–‡ä»¶ï¼Œä½¿ç”¨çŽ°æœ‰QAé“¾")

    # ç”Ÿæˆé—®ç­”ç»“æžœï¼ˆåŒæ­¥è°ƒç”¨ï¼Œè¿™é‡Œå¦‚æžœgen_responseæ”¯æŒå¼‚æ­¥è¯·æ”¹æˆawaitï¼‰
    print(f"æ”¶åˆ°æ–‡æœ¬ï¼Œç”Ÿæˆå›žç­”ï¼š{text}")
    answer = gen_response(
        chain=_qa_chain,
        input=text,
        chat_history=_chat_history,
    )
    # å¦‚æžœansweræ˜¯ç”Ÿæˆå™¨ï¼Œåˆå¹¶æˆå­—ç¬¦ä¸²
    if hasattr(answer, '__iter__') and not isinstance(answer, str):
        output = "".join(answer)
    else:
        output = answer
    _chat_history.append(("human", text))
    _chat_history.append(("ai", output))

    print(f"ç”Ÿæˆå›žç­”: {output}")

    # åˆæˆè¯­éŸ³ï¼Œè¾“å‡ºè·¯å¾„å›ºå®š
    audio_file_path = "data/audio.mp3"
    # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªå®šä¹‰éŸ³è‰²
    user_voice_path = "/root/codespace/data/user_voice.npy"
    prompt_tokens_path = [user_voice_path if os.path.exists(user_voice_path) else "/root/codespace/TTS/fake.npy"]

    try:
        text_to_speech(output, prompt_tokens_path=prompt_tokens_path, output_audio_path=audio_file_path)
    except Exception as e:
        print(f"è¯­éŸ³åˆæˆå¤±è´¥: {e}")
        # æ¸…ç†CUDAç¼“å­˜
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        # é‡è¯•ä¸€æ¬¡
        try:
            text_to_speech(output, prompt_tokens_path=prompt_tokens_path, output_audio_path=audio_file_path)
        except Exception as e:
            print(f"è¯­éŸ³åˆæˆé‡è¯•å¤±è´¥: {e}")
            raise
    print(f"è¯­éŸ³åˆæˆå®Œæˆï¼Œè·¯å¾„ï¼š{audio_file_path}")

    # å‘é€éŸ³é¢‘æ–‡ä»¶
    await send_audio_to_cloud(audio_file_path)

async def receive_text_from_pi():
    uri = "ws://182.92.128.19:8000/ws/text"
    async with websockets.connect(uri) as websocket:
        print("è¿žæŽ¥æ ‘èŽ“æ´¾æ–‡å­—è¾“å…¥ WebSocket æˆåŠŸï¼Œç­‰å¾…æ¶ˆæ¯...")
        try:
            while True:
                message = await websocket.recv()
                print(f"æ”¶åˆ°æ ‘èŽ“æ´¾æ¶ˆæ¯: {message}")

                # æ”¶åˆ°æ–‡å­—åŽå¤„ç†ç”Ÿæˆè¯­éŸ³å¹¶å‘é€
                await process_text(message)
        except asyncio.TimeoutError:
            print("Sending keepalive ping...")
            await websocket.ping() # å‘é€å¿ƒè·³åŒ…ï¼Œä¿æŒè¿žæŽ¥çŠ¶æ€
        except websockets.exceptions.ConnectionClosed as e:
            print(f"è¿žæŽ¥å…³é—­: {e}")
        except Exception as e:
            print(f"å¼‚å¸¸: {e}")

if __name__ == "__main__":
    initialize_models()
    asyncio.run(receive_text_from_pi())
