import asyncio
import websockets
import os
import glob
from chat_with_wenwu.streamlit_app import get_qa_history_chain, gen_response, get_available_models
from TTS.tts import text_to_speech, initialize_tts_models
from pathlib import Path
from weather_mcp.MCPClient import MCPClient

# ç¼“å­˜é—®ç­”é“¾
_qa_chain = None
_selected_model = "qwen2.5-coder-32b-instruct"
_chat_history = []

def clean_audio_data_folder():
    audio_files = glob.glob("data/*.wav")
    for f in audio_files:
        try:
            os.remove(f)
        except Exception as e:
            print(f"Failed to delete {f}: {e}")

async def send_audio_to_cloud(audio_path):
    uri = "ws://182.92.128.19:8000/ws/upload"
    CHUNK_SIZE = 512 * 1024

    try:
        async with websockets.connect(uri, max_size=50 * 1024 * 1024) as websocket:
            file_size = os.path.getsize(audio_path)
            await websocket.send(f"{os.path.basename(audio_path)},{file_size}")

            with open(audio_path, "rb") as f:
                bytes_sent = 0
                while bytes_sent < file_size:
                    chunk = f.read(CHUNK_SIZE)
                    await websocket.send(chunk)
                    bytes_sent += len(chunk)

        print(f"âœ… éŸ³é¢‘å·²å‘é€è‡³æœåŠ¡å™¨ï¼š{audio_path}")

        # å‘é€æˆåŠŸï¼Œåˆ é™¤éŸ³é¢‘
        os.remove(audio_path)
        print(f"ğŸ—‘ï¸ å·²åˆ é™¤æœ¬åœ°éŸ³é¢‘æ–‡ä»¶ï¼š{audio_path}")
    except asyncio.TimeoutError:
        print("Sending keepalive ping...")
        await websocket.ping() # å‘é€å¿ƒè·³åŒ…ï¼Œä¿æŒè¿æ¥çŠ¶æ€
    except Exception as e:
        print(f"âŒ WebSocket å‘é€å¤±è´¥ï¼š{e}ï¼ˆéŸ³é¢‘ä¿ç•™ä»¥ä¾¿é‡è¯•ï¼‰")

def get_available_pdfs():
    """è·å–knowledge_dbç›®å½•ä¸‹æ‰€æœ‰PDFæ–‡ä»¶"""
    knowledge_dir = Path("/root/codespace/data/knowledge_db")
    knowledge_dir.mkdir(parents=True, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
    return list(knowledge_dir.glob("*.pdf"))

def load_qa_chain(model="qwen2.5-coder-32b-instruct", vectordb_path=None):
    global _qa_chain, _vectordb_path
    _qa_chain = get_qa_history_chain(model=model, vectordb_path=vectordb_path)
    print(f"QA chain loaded with model {model}")

def initialize_models():
    print("åˆå§‹åŒ– TTS æ¨¡å‹...")
    # è®¾ç½®å†…å­˜ä¼˜åŒ–é…ç½®
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
    # åˆå§‹åŒ–TTSæ¨¡å‹ï¼Œå…³é—­halfç²¾åº¦ä»¥å‡å°‘å†…å­˜é—®é¢˜
    initialize_tts_models(half=False)
    clean_audio_data_folder()
    load_qa_chain(model=_selected_model, vectordb_path=None)
async def get_weather_info(city: str) -> str:
    """è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯"""
    client = MCPClient()
    try:
        # è®¾ç½®è¶…æ—¶ä¸º30ç§’
        await asyncio.wait_for(
            client.connect_to_server("weather_mcp/server.py"),
            timeout=30.0
        )
        weather_info = await asyncio.wait_for(
            client.process_query(f"{city}çš„å¤©æ°”"),
            timeout=30.0
        )
        return weather_info
    except asyncio.TimeoutError:
        return "å¤©æ°”æŸ¥è¯¢è¶…æ—¶ï¼Œè¯·ç¨åå†è¯•"
    except Exception as e:
        return f"è·å–å¤©æ°”ä¿¡æ¯å¤±è´¥: {str(e)}"
    finally:
        try:
            await asyncio.wait_for(client.cleanup(), timeout=5.0)
        except:
            pass  # ç¡®ä¿æ— è®ºå¦‚ä½•éƒ½èƒ½ç»§ç»­
async def process_text(input):
    # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„PDFçŸ¥è¯†åº“
    pdf_files = get_available_pdfs()
    if pdf_files:
        pdf_file = pdf_files[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªPDFæ–‡ä»¶
        pdf_path = str(pdf_file)
        persist_dir = Path("/root/codespace/data/vector_db") / pdf_file.stem
        persist_dir.mkdir(parents=True, exist_ok=True)
        vectordb_path = (pdf_path, str(persist_dir))
        
            
        print(f"æ£€æµ‹åˆ°çŸ¥è¯†åº“æ–‡ä»¶ï¼š{pdf_path}ï¼Œæ›´æ–°QAé“¾...")
        try:
            load_qa_chain(model=_selected_model, vectordb_path=vectordb_path)
        except Exception as e:
            print(f"åŠ è½½QAé“¾å¤±è´¥: {e}")
            print("å°†ä½¿ç”¨æ— çŸ¥è¯†åº“æ¨¡å¼ç»§ç»­...")
            load_qa_chain(model=_selected_model, vectordb_path=None)
    else:
        print("æœªæ£€æµ‹åˆ°çŸ¥è¯†åº“æ–‡ä»¶ï¼Œä½¿ç”¨ç°æœ‰QAé“¾")

    # ç”Ÿæˆé—®ç­”ç»“æœï¼ˆåŒæ­¥è°ƒç”¨ï¼Œè¿™é‡Œå¦‚æœgen_responseæ”¯æŒå¼‚æ­¥è¯·æ”¹æˆawaitï¼‰
    print(f"æ”¶åˆ°æ–‡æœ¬ï¼Œç”Ÿæˆå›ç­”ï¼š{input}")
    # ç®€å•çš„ä¸­æ–‡åŸå¸‚ååˆ°è‹±æ–‡æ˜ å°„
    city_mapping = {
        "åŒ—äº¬": "Beijing",
        "ä¸Šæµ·": "Shanghai",
        "å¹¿å·": "Guangzhou",
        "æ·±åœ³": "Shenzhen",
        "æ­å·": "Hangzhou",
        "æˆéƒ½": "Chengdu"
    }
    
    # æ£€æŸ¥æ˜¯å¦è¯¢é—®å¤©æ°”ä¸”å¯ç”¨äº†MCPæœåŠ¡
    try:
        import json
        with open("weather_mcp/config.json") as f:
            config = json.load(f)
        use_weather_mcp = config.get("use_weather_mcp", False)
    except:
        use_weather_mcp = False
    
    if "å¤©æ°”" in input and use_weather_mcp:
        try:
            # è·å–å½“å‰äº‹ä»¶å¾ªç¯
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # å¦‚æœäº‹ä»¶å¾ªç¯å·²åœ¨è¿è¡Œï¼Œåˆ›å»ºæ–°çº¿ç¨‹å¤„ç†å¼‚æ­¥è°ƒç”¨
                from concurrent.futures import ThreadPoolExecutor
                with ThreadPoolExecutor() as executor:
                    weather_info = executor.submit(
                        lambda: asyncio.run(get_weather_info(input))
                    ).result()
            else:
                # å¦åˆ™ç›´æ¥è¿è¡Œ
                weather_info = loop.run_until_complete(get_weather_info(input))
            
            # å°†å¤©æ°”ä¿¡æ¯åˆå¹¶åˆ°è¾“å…¥ä¸­
            input = f"{input}\n{weather_info}"
        except Exception as e:
            print(f"å¤©æ°”æŸ¥è¯¢å¼‚å¸¸: {e}")
            input = f"{input}\n[å¤©æ°”æŸ¥è¯¢æœåŠ¡æš‚æ—¶ä¸å¯ç”¨]"

    answer = gen_response(
        chain=_qa_chain,
        input=input,
        chat_history=_chat_history,
    )
    # å¦‚æœansweræ˜¯ç”Ÿæˆå™¨ï¼Œåˆå¹¶æˆå­—ç¬¦ä¸²
    if hasattr(answer, '__iter__') and not isinstance(answer, str):
        output = "".join(answer)
    else:
        output = answer
    _chat_history.append(("human", input))
    _chat_history.append(("ai", output))

    print(f"ç”Ÿæˆå›ç­”: {output}")

    # åˆæˆè¯­éŸ³ï¼Œè¾“å‡ºè·¯å¾„å›ºå®š
    audio_file_path = "data/audio.mp3"
    # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªå®šä¹‰éŸ³è‰²
    user_voice_path = "/root/codespace/data/user_voice.npy"
    prompt_tokens_path = [user_voice_path if os.path.exists(user_voice_path) else "/root/codespace/TTS/fake.npy"]

    try:
        text_to_speech(output, prompt_tokens_path=prompt_tokens_path, output_audio_path=audio_file_path)
    except Exception as e:
        print(f"è¯­éŸ³åˆæˆå¤±è´¥: {e}")
        # æ¸…ç†CUDAç¼“å­˜
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        # é‡è¯•ä¸€æ¬¡
        try:
            text_to_speech(output, prompt_tokens_path=prompt_tokens_path, output_audio_path=audio_file_path)
        except Exception as e:
            print(f"è¯­éŸ³åˆæˆé‡è¯•å¤±è´¥: {e}")
            raise
    print(f"è¯­éŸ³åˆæˆå®Œæˆï¼Œè·¯å¾„ï¼š{audio_file_path}")

    # å‘é€éŸ³é¢‘æ–‡ä»¶
    await send_audio_to_cloud(audio_file_path)

async def receive_text_from_pi():
    uri = "ws://182.92.128.19:8000/ws/text"
    async with websockets.connect(uri) as websocket:
        print("è¿æ¥æ ‘è“æ´¾æ–‡å­—è¾“å…¥ WebSocket æˆåŠŸï¼Œç­‰å¾…æ¶ˆæ¯...")
        try:
            while True:
                message = await websocket.recv()
                print(f"æ”¶åˆ°æ ‘è“æ´¾æ¶ˆæ¯: {message}")

                # æ”¶åˆ°æ–‡å­—åå¤„ç†ç”Ÿæˆè¯­éŸ³å¹¶å‘é€
                await process_text(message)
        except asyncio.TimeoutError:
            print("Sending keepalive ping...")
            await websocket.ping() # å‘é€å¿ƒè·³åŒ…ï¼Œä¿æŒè¿æ¥çŠ¶æ€
        except websockets.exceptions.ConnectionClosed as e:
            print(f"è¿æ¥å…³é—­: {e}")
        except Exception as e:
            print(f"å¼‚å¸¸: {e}")

if __name__ == "__main__":
    initialize_models()
    asyncio.run(receive_text_from_pi())
